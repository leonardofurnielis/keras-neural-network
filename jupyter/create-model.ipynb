{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment AnalysesÂ¶\n",
    "\n",
    "This notebook uses Kaggle dataset (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
    "\n",
    "1. Import dataset\n",
    "3. Data preparation\n",
    "3. Creation of training and test dataset\n",
    "4. Model training\n",
    "5. Save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "df = pd.read_csv('../datasets/imdb-dataset.csv', delimiter=',')\n",
    "df = df.head(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Data preparation\n",
    "Data preparation using the following Text Feature Engineering techniques:\n",
    "\n",
    "1. Tonkenization\n",
    "2. Removes stop words\n",
    "3. Stemming text (porter)\n",
    "4. Joining words (tokens) into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    \"\"\"Identify tokens in a row\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: text splited in tokens\n",
    "    \"\"\"    \n",
    "    source = row[0]\n",
    "    tokens = word_tokenize(source)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(row):\n",
    "    \"\"\"Remove stop words from text\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: list of tokens without stop words\n",
    "    \"\"\"    \n",
    "    source_tokenization = row[2]\n",
    "    stop = [w for w in source_tokenization if not w in stop_words]\n",
    "    return (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_porter(row):\n",
    "    \"\"\"Execute steamming porter\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        list: list of tokens with steamming.\n",
    "    \"\"\"      \n",
    "    my_list = row[2]\n",
    "    stemmed_list = [porter_stemmer.stem(word) for word in my_list]\n",
    "    return (stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    \"\"\"Join tokens in a single string\n",
    "    Args:\n",
    "        row (list): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        str: text of joined tokens\n",
    "    \"\"\"      \n",
    "    my_list = row[2]\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    \"\"\"Execute text feature engineering (TFE)\n",
    "    Args:\n",
    "        df (dataframe): row of dataframe\n",
    "    \n",
    "    Returns:\n",
    "        df: New df post text feature engineering (TFE)\n",
    "    \"\"\" \n",
    "    print('Tokenization ...')\n",
    "    df['text1'] = df.apply(identify_tokens, axis=1)\n",
    "    print('Removing stop words ...')\n",
    "    df['text1'] = df.apply(remove_stops, axis=1)\n",
    "    print('Stemming (porter) ...')\n",
    "    df['text1'] = df.apply(stem_porter, axis=1)\n",
    "    print('Joining words ...')\n",
    "    df['clean_text'] = df.apply(rejoin_words, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_processing(df)\n",
    "df['clean_text'] = df['clean_text'].str.lower()\n",
    "\n",
    "X = df['clean_text']\n",
    "Y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Creation of training and test dataset\n",
    "NOTE: Test dataset (30%) and Training dataset (70%) balanced (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=48,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3),\n",
    "                        sublinear_tf=True,\n",
    "                        max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Machine Learning or Deep Learning models uses numeric values input. The Tf-Idf Text Feature Engineering (TFE) process will be used to transform the texts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_test_tf = vectorizer.transform(X_test)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(list(Y_train))\n",
    "Y_train_le = le.transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))\n",
    "\n",
    "num_class = Y.value_counts().shape\n",
    "input_shape = X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train_label_keras = to_categorical(Y_train_le)\n",
    "Y_test_label_keras = to_categorical(Y_test_le)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(2, activation='relu', input_shape=(input_shape[1], )))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='relu'))\n",
    "network.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='sigmoid'))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(num_class[0], activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='adamax',\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(X_train_tf.toarray(),\n",
    "            Y_train_label_keras,\n",
    "            verbose=1,\n",
    "            epochs=50,\n",
    "            validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 - Save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('../models/neural_network.h5')\n",
    "pickle.dump(vectorizer, open('../models/vectorizer.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
