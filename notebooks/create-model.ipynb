{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB reviews sentiment analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Kaggle dataset (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Explore and prepare training data](#explore_prepare_data)\n",
    "1. [Create train and test dataset](#train_test_set)\n",
    "1. [Train the model](#train_model)\n",
    "1. [Save the model](#save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 17:09:57.730041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_prepare_data\"></a>\n",
    "## 1. Explore and prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# This try-except block addresses SSL certificate verification issues.\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to ../nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append('../nltk_data')\n",
    "nltk.download(['stopwords', 'punkt'], download_dir='../nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "df = pd.read_csv('../data/imdb_dataset.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring and preparing data\n",
    "\n",
    "In this step you will prepare data for training a model. Using the following Text Feature Engineering techniques:\n",
    "\n",
    "1. Tonkenization         \n",
    "2. Removes stop words\n",
    "3. Stemming text (porter)\n",
    "4. Joining words (tokens) into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '<your-directory>/tensorflow-keras-container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import tokenization, remove_stop_words, stem_porter, rejoin_words, word2vec_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review']\n",
    "\n",
    "input_tokens = reviews.apply(tokenization)\n",
    "input_tokens = input_tokens.apply(remove_stop_words)\n",
    "input_tokens = input_tokens.apply(stem_porter)\n",
    "input_text_cleaned = input_tokens.apply(rejoin_words)\n",
    "\n",
    "df['cleaned_text'] = input_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_test_set\"></a>\n",
    "## 2. Create train and test dataset\n",
    "\n",
    "NOTE: Test dataset (30%) and Training dataset (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "Y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=48,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 3),\n",
    "                        sublinear_tf=True,\n",
    "                        max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Machine Learning or Deep Learning models uses numeric values input. The Tf-Idf Text Feature Engineering (TFE) process will be used to transform the texts into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(Y.value_counts().shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Y_train_le = le.fit_transform(list(Y_train))\n",
    "Y_test_le = le.transform(list(Y_test))\n",
    "\n",
    "num_class = Y.value_counts().shape\n",
    "input_shape = X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_model\"></a>\n",
    "## 3. Train the model\n",
    "\n",
    "Create a Keras neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train_label_keras = to_categorical(Y_train_le)\n",
    "Y_test_label_keras = to_categorical(Y_test_le)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(2, activation='relu', input_shape=(input_shape[1], )))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='relu'))\n",
    "network.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(5, activation='sigmoid'))\n",
    "network.add(layers.Dropout(0.4))\n",
    "\n",
    "network.add(layers.Dense(num_class[0], activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='adamax',\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(X_train_tf.toarray(),\n",
    "            Y_train_label_keras,\n",
    "            verbose=1,\n",
    "            epochs=50,\n",
    "            validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the model\n",
    "\n",
    "Save Keras model and TF-IDF text vectorizer to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('../models/sentiment_network_model.keras')\n",
    "pickle.dump(tfidf_vectorizer, open('../models/tfidf_text_vectorizer.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
